{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyBKT.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(seed = 60, num_fits = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fetch_dataset(\"https://raw.githubusercontent.com/lishaparmar13/H5P-Enhanced-BKT/main/Under16-SS.csv\",'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4= pd.read_csv(r\"Under16-SS.csv\", encoding ='latin')\n",
    "df4.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df4, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data = df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data_path = 'Sepcolumn.csv')\n",
    "print(\"Fitted Skills:\\n%s\" % '\\n'.join(model.coef_.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with the default RMSE then specify AUC.\n",
    "model.fit(data = df4)\n",
    "training_rmse = model.evaluate(data = df4)\n",
    "training_auc = model.evaluate(data = df4, metric = 'auc')\n",
    "print(\"Training RMSE: %f\" % training_rmse)\n",
    "print(\"Training AUC: %f\" % training_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define own metric\n",
    "def mae(true_vals, pred_vals):\n",
    "  \"\"\" Calculates the mean absolute error. \"\"\"\n",
    "  return np.mean(np.abs(true_vals - pred_vals))\n",
    "\n",
    "training_mae = model.evaluate(data = df4, metric = mae)\n",
    "print(\"Training MAE: %f\" % training_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(seed = 60, num_fits = 1)\n",
    "model.fit(data = train_df)\n",
    "preds = model.predict(data = test_df)\n",
    "preds[['Anon Student Id', 'KC(Default)', 'Correct First Attempt', \n",
    "       'correct_predictions', 'state_predictions']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's sanity check that we have only trained on the skills that we \n",
    "# specified in the call to fit! Note that while it is possible for a \n",
    "# BKT prediction to be 0.5 exactly, it is unlikely.\n",
    "preds[preds['correct_predictions'] != 0.5]['KC(Default)'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use model.evaluate to accomplish the same thing!\n",
    "# You should receive an RMSE that is identical to the above\n",
    "# manually calculated RMSE.\n",
    "model.evaluate(data = df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(data=df4, metric=['rmse','accuracy','auc','mean_absolute_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(seed = 60, num_fits = 1)\n",
    "# Note that folds is an optional parameter as well as the model \n",
    "# variant, seed, and crossvalidated metric.\n",
    "# By default, we crossvalidate on all skills separately.\n",
    "model.crossvalidate(data= df4, folds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(seed = 60, num_fits = 1)\n",
    "# Try this with a different skill or metric by replacing the lines below.\n",
    "skill = 'Algebra'\n",
    "metric = 'rmse'\n",
    "\n",
    "simple_cv = model.crossvalidate(data = df4, skills = skill, \n",
    "                                metric = metric)\n",
    "simple_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(seed = 60, num_fits = 1)\n",
    "multigs_cv = model.crossvalidate(data = df4, skills = skill,\n",
    "                                 multigs = True, metric = metric)\n",
    "multigs_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(seed = 60, num_fits = 1)\n",
    "skill = 'Algebra'\n",
    "\n",
    "multilearn_cv = model.crossvalidate(df4, skills = skill,\n",
    "                                    multilearn =\"question_id\", metric = metric)\n",
    "multilearn_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(seed = 60, num_fits = 1)\n",
    "# The multiprior model generates different priors based on the first \n",
    "# response of each student.\n",
    "multiprior_cv = model.crossvalidate(data = df4, skills = skill,\n",
    "                                    multiprior = True, metric = metric,\n",
    "                                    folds = 3)\n",
    "\n",
    "model = Model(seed = 60, num_fits = 1)\n",
    "multipair_cv = model.crossvalidate(data = df4, skills = skill,\n",
    "                                   multipair = True, metric = metric,\n",
    "                                   folds = 3)\n",
    "pd.concat([multiprior_cv, multipair_cv], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(seed = 60, num_fits = 1)\n",
    "# We combine the fifth parameter, forgets, with the previous multilearn\n",
    "# and multiguess/slip models for a combo model.\n",
    "combo_cv = model.crossvalidate(data = df4, skills = skill,\n",
    "                               forgets = True, multilearn = True, \n",
    "                               multigs = True, metric = metric)\n",
    "combo_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(seed = 60, num_fits = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill = 'Geometry'\n",
    "\n",
    "model.coef_ = {skill: {'prior': 1e-40}}\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with the pre-initialized parameters.\n",
    "model.fit(data = df4, multigs = True)\n",
    "low_prior_auc = model.evaluate(data= df4, metric = 'auc')\n",
    "\n",
    "# We can obtain the prior value by indexing into the model.coef_\n",
    "# dictionary with the skill and parameter names. \n",
    "print(\"Fitted Prior Value: %f\" % model.coef_[skill]['prior'])\n",
    "print(\"Training AUC: %f\" % low_prior_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the prior to be more reasonable.\n",
    "model.coef_ = {skill: {'prior': 0.5}}\n",
    "model.fit(data= df4, multigs = True)\n",
    "normal_prior_auc = model.evaluate(data= df4, metric = 'auc')\n",
    "\n",
    "# Print the fitted prior value and RMSE.\n",
    "print(\"Fitted Prior Value: %f\" % model.coef_[skill]['prior'])\n",
    "print(\"Training AUC: %f\" % normal_prior_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data= df4, skills=skill,\n",
    "          forgets = True, multilearn = True, \n",
    "          multigs = True)\n",
    "params= model.params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will get warnings for using indexing past lexsort. That's fine,\n",
    "# and we will disable these warnings.\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "#Model Intialization\n",
    "#model = Model(seed = 60, num_fits = 1)\n",
    "#model.fit(data = df4,\n",
    "#          forgets = True, multilearn = True, \n",
    "#          multigs = True)\n",
    "\n",
    "# Plot the learns, forgets, slips and guesses for each of the classes.\n",
    "#params = model.params()\n",
    "plt.figure(figsize = (12, 6))\n",
    "plt.plot(params.loc[(skill, 'guesses')], label = 'Guesses')\n",
    "plt.plot(params.loc[(skill, 'learns')], label = 'Learns')\n",
    "plt.plot(params.loc[(skill, 'forgets')], label = 'Forgets')\n",
    "plt.plot(params.loc[(skill, 'slips')], label = 'Slips')\n",
    "plt.xlabel('Template ID')\n",
    "plt.ylabel('Rate')\n",
    "plt.title('BKT Parameters per Template ID Class for Under16 Dataset')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set options to display all rows and columns\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data = df4, \n",
    "          forgets = True, multilearn = True, \n",
    "          multigs = True)\n",
    "params= model.params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
